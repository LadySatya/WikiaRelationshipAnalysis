# Wikia Character Relationship Analysis Project

## Project Overview
A modular system to webcrawl wikia sites, extract character information, and use LLM analysis to discover and visualize relationships between characters.

## Current Status

### âœ… Phase 1 Complete - Web Crawler Implementation
The core web crawling infrastructure is now fully implemented and tested:

#### **Implemented Components**
- **Rate Limiting System** - Ethical crawling with per-domain throttling and robots.txt compliance
- **Session Management** - Robust HTTP session handling with retries and backoff
- **URL Management** - Queue-based URL processing with deduplication and priority handling
- **Content Extraction** - BeautifulSoup-based HTML parsing and text extraction
- **Wikia-Specific Parsing** - Specialized content filtering for Wikia/Fandom sites
- **Link Discovery** - Intelligent relationship-aware link discovery and prioritization
- **Content Filtering** - Removal of navigation elements while preserving main content
- **Persistence Layer** - File-based storage with project organization
- **Domain Validation** - Strict filtering to ensure single-wikia scope

#### **Key Features**
- **403+ Passing Tests** - Comprehensive test coverage for all components
- **Manual Testing Interface** - Interactive test script (`test_crawler.py`) for development
- **Project-Based Storage** - Isolated data directories for each wikia analysis
- **Configuration System** - YAML-based configuration with hierarchical overrides
- **Error Handling** - Robust error handling with exponential backoff for failed requests

## System Architecture (Modular Design)

### 1. **Web Crawler Module** (`src/crawler/`) - âœ… IMPLEMENTED
- **Purpose**: Extract content from wikia sites with ethical crawling practices
- **Components**:
  - `WikiaCrawler`: Main orchestrator with session management and state persistence
  - `PageExtractor`: Extracts structured content from wikia pages
  - `WikiaParser`: Wikia/Fandom-specific content parsing and namespace handling
  - `LinkDiscoverer`: Relationship-aware character/location page discovery
  - `ContentFilter`: Filters wikia navigation while preserving main content
  - `SessionManager`: HTTP session handling with timeout and retry logic
  - `URLManager`: Queue management with deduplication and priority handling
  - `RateLimiter`: Per-domain request throttling with burst protection
  - `ContentSaver`: File-based storage with URL-to-filename mapping

### 2. **RAG Processor Module** (`src/processor/`) - ðŸ“‹ NEXT (Phase 2)
- **Purpose**: Index crawled data and extract character information using Retrieval Augmented Generation (RAG)
- **Approach**: Instead of processing every page individually, build a searchable vector database and query it intelligently
- **Components**:
  - **Indexing Pipeline**:
    - `ContentChunker`: Splits pages into semantic chunks (~500 chars) for embedding
    - `EmbeddingGenerator`: Generates vector embeddings (Voyage AI voyage-3-lite)
    - `VectorStore`: ChromaDB-based persistent vector database
  - **RAG Query System**:
    - `RAGRetriever`: Semantic search to find relevant chunks
    - `QueryEngine`: Combines retrieval + Claude LLM to answer questions
  - **Character Analysis**:
    - `CharacterExtractor`: Discovers all characters using RAG queries
    - `ProfileBuilder`: Builds comprehensive character profiles from retrieved context

**Why RAG?**
- **Scalable**: Handles thousands of pages efficiently
- **Cost-Effective**: Only pays for relevant context (~$1.70 for 100 pages + 50 characters)
- **Accurate**: Semantic search finds relevant information across entire corpus
- **Source Tracking**: Know which chunks support each extracted fact
- **Flexible**: Can answer arbitrary questions about the wiki
- **Better Reasoning**: Claude 3.5 Haiku provides superior analysis quality

**Planned CLI**:
```bash
python main.py index <project_name>                    # Index crawled data
python main.py discover-characters <project_name>      # Find all characters
python main.py build-profile <project_name> "Aang"     # Build character profile
python main.py query <project_name> "Who is Katara?"   # Query the RAG system
```

### 4. **Relationship Mapping Module** (`mapper/`) - ðŸ”„ PLANNED
- **Purpose**: Process and structure relationship data
- **Components**:
  - `RelationshipGraph`: Build graph structure of relationships
  - `RelationshipClassifier`: Categorize relationship types
  - `ConfidenceScorer`: Assign confidence scores to relationships
  - `ConflictResolver`: Handle conflicting relationship information

### 5. **Visualization Module** (`visualizer/`) - ðŸ”„ PLANNED
- **Purpose**: Generate charts and visual representations
- **Components**:
  - `NetworkGraphGenerator`: Creates interactive network graphs
  - `RelationshipCharts`: Generates various chart types
  - `ExportManager`: Exports to different formats (JSON, CSV, SVG)
  - `InteractiveViewer`: Web-based relationship explorer

### 6. **Data Storage Module** (`storage/`) - âœ… PARTIAL (File-based implemented)
- **Purpose**: Persist and manage data
- **Components**:
  - `ContentSaver`: âœ… File-based storage with project organization
  - `CrawlState`: âœ… Session persistence for resumable crawls
  - `DatabaseManager`: ðŸ”„ Handle database operations (planned)
  - `CacheManager`: ðŸ”„ Cache frequently accessed data (planned)
  - `BackupManager`: ðŸ”„ Data backup and recovery (planned)

## Technology Stack

- **Backend**: Python 3.13+ 
- **Web Crawling**: BeautifulSoup4 + aiohttp (async HTTP)
- **Testing**: pytest with comprehensive coverage (403+ tests)
- **Data Format**: JSON for structured data, YAML for configuration
- **Storage**: File-based with project isolation
- **RAG System**: ChromaDB (vector database) + Voyage AI embeddings (Phase 2)
- **LLM Integration**: Anthropic Claude API (Claude 3.5 Haiku for RAG queries)
- **Visualization**: NetworkX + Plotly/D3.js (planned for Phase 3)
- **API**: FastAPI for web interface (planned)
- **Configuration**: YAML-based config files with hierarchical overrides

## Implementation Progress

### âœ… Phase 1: Web Crawler Foundation (COMPLETED)
1. âœ… Set up project structure and configuration system
2. âœ… Implement comprehensive web crawler with rate limiting
3. âœ… Create file-based data models and storage system
4. âœ… Build text extraction and content filtering pipeline
5. âœ… Add domain validation and single-wikia scope enforcement
6. âœ… Implement comprehensive test suite (403+ tests)
7. âœ… Create manual testing interface for development

### ðŸ“‹ Phase 2: RAG-Based Character Analysis (NEXT)
**Goal**: Build a searchable knowledge base from crawled data and extract character profiles using RAG

#### Phase 2a: Indexing (TDD Implementation)
1. âœ… Install RAG dependencies (`pip install -e ".[dev,rag]"` - chromadb, anthropic, voyageai)
2. Implement `ContentChunker` - Split pages into semantic chunks with overlap
3. Implement `EmbeddingGenerator` - Generate vector embeddings (Voyage AI/local)
4. Implement `VectorStore` - ChromaDB integration with persistence
5. Test end-to-end indexing on test_resume project (5 pages)

#### Phase 2b: Character Discovery (Integration Tests)
6. Implement `RAGRetriever` - Semantic search functionality
7. Implement `QueryEngine` - RAG query interface (retrieval + Claude)
8. Implement `CharacterExtractor` - Multi-query character discovery
9. Implement `ProfileBuilder` - Build character profiles from RAG context
10. Validate on Avatar wiki data (accuracy > 90%, powered by Claude)

#### Phase 2c: CLI & Documentation
11. Add CLI commands (index, discover-characters, build-profile, query)
12. Create `config/processor_config.yaml` with RAG settings
13. Update documentation with RAG workflow and examples
14. Cost analysis and optimization (<$1 per project)

**Success Criteria**:
- Index 100+ pages into ChromaDB successfully
- Discover 20+ characters from test corpus
- Build accurate profiles with relationship extraction (Claude-powered)
- Total processing cost < $2 per project (using Claude 3.5 Haiku)
- Ready for Phase 3 (relationship graph visualization)

### ðŸ”„ Phase 3: Relationship Graph Building (PLANNED)
1. Build graph structure from character profiles
2. Implement relationship classification and scoring
3. Create graph analysis tools (community detection, centrality)
4. Export graph data for visualization

### ðŸ”„ Phase 4: Visualization & Interface (PLANNED)
1. Implement network graph generation
2. Create interactive web interface
3. Build export functionality
4. Add filtering and search capabilities

### ðŸ”„ Phase 5: Optimization & Deployment (PLANNED)
1. Performance optimization and caching
2. Database integration and migration
3. Enhanced error handling and monitoring
4. Deployment setup and CI/CD

## Project Structure
```
WikiaAnalysis/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ crawler/
â”‚       â”œâ”€â”€ core/           # Main crawler orchestration
â”‚       â”œâ”€â”€ extraction/     # Content parsing and link discovery  
â”‚       â”œâ”€â”€ persistence/    # Data storage and state management
â”‚       â”œâ”€â”€ rate_limiting/  # Ethical crawling controls
â”‚       â””â”€â”€ utils/          # Content filtering and utilities
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_crawler/      # Comprehensive test suite (403+ tests)
â”œâ”€â”€ config/                # Configuration templates (planned)
â”œâ”€â”€ data/                  # Project data storage
â”œâ”€â”€ scripts/               # CLI interfaces
â”œâ”€â”€ test_crawler.py        # Manual testing interface
â”œâ”€â”€ main.py               # Main application entry point
â””â”€â”€ pyproject.toml        # Modern Python packaging
```

## Key Features
- **âœ… Ethical Crawling**: Rate limiting, robots.txt compliance, and domain validation
- **âœ… Robust Architecture**: Comprehensive error handling with exponential backoff
- **âœ… Project Isolation**: Separate data directories for each wikia analysis
- **âœ… Relationship-Aware**: Intelligent link discovery prioritizing character connections
- **âœ… Wikia-Specialized**: Custom parsing for Fandom/Wikia site structures
- **âœ… Test-Driven**: 403+ comprehensive tests ensuring reliability
- **âœ… Developer-Friendly**: Manual testing interface and detailed logging
- **ðŸ”„ Configurable**: Easy adaptation to different wikia sites (planned)
- **ðŸ”„ Scalable**: Designed to handle large wikia sites (planned)
- **ðŸ”„ Interactive**: Web-based visualization and exploration (planned)

## Getting Started

### Prerequisites
```bash
Python 3.13+
pip install -e ".[dev]"  # Install with development dependencies
```

### Live Testing Results âœ…

**Wikia Content Extraction Successfully Implemented** (September 2024):

Complete end-to-end testing validated full content extraction functionality:

#### Test Results Summary:
- **âœ… Configuration System**: Loaded YAML config successfully with rate limiting (1.0s delay, 60 req/min)
- **âœ… Project Structure**: Created proper directory hierarchy with 14 subdirectories
- **âœ… URL Validation**: Successfully validated wikia URLs and domain filtering
- **âœ… Rate Limiting**: Infrastructure working (observable delays between requests)
- **âœ… Error Handling**: Graceful failure handling with proper HTTP status management
- **âœ… Session Management**: HTTP sessions with proper cleanup
- **âœ… Content Extraction**: Real content extraction from 55+ pages with rich character data
- **âœ… Meaningful Filenames**: Human-readable names like `Tenzin_20250911.json`
- **âœ… Link Discovery**: 527 URLs discovered from just 2 pages

#### Current Working Commands:
```bash
# Working CLI (ready to use now)
python main.py crawl my_project https://avatar.fandom.com/wiki/Avatar_Wiki --max-pages 5
python main.py status my_project
python main.py list

# Integration testing (for development)
python test_crawl.py  # End-to-end validation test
python test_resume.py  # Resume functionality test
```

#### Actual Test Results:
- **Pages crawled**: 55+ real pages with full content
- **Content types**: Characters, articles, disambiguation pages  
- **Filenames**: `Tenzin_20250911.json`, `United_Republic_Council_20250911.json`
- **Rich data**: Full biographies, abilities, relationships, infobox data
- **Link discovery**: 527 URLs from 2 pages, 2580 URLs from 50 pages
- **Error rate**: 0% on successful extractions

### Quick Start - Ready to Use Now âœ…

#### Working CLI Commands:
```bash
# Crawl any wikia site
python main.py crawl my_project https://avatar.fandom.com/wiki/Category:Characters --max-pages 10

# Check project status and content
python main.py status my_project
python main.py list

# View extracted content preview
python main.py view my_project

# Verify installation works
python -m pytest tests/test_crawler/utils/test_url_utils.py::TestURLUtilsValidation::test_validate_malformed_url -v
```

### Development Workflow

#### Testing Commands:
```bash
# Run full test suite (slow - 400+ tests)
python -m pytest tests/test_crawler/ -v

# Run specific component tests (faster)
python -m pytest tests/test_crawler/rate_limiting/ -v
python -m pytest tests/test_crawler/core/ -v
python -m pytest tests/test_crawler/utils/ -v

# Single test for quick verification
python -m pytest tests/test_crawler/utils/test_url_utils.py::TestURLUtilsValidation::test_validate_malformed_url -v

# Test with coverage
python -m pytest --cov=src --cov-report=html
```

#### Code Quality Commands:
```bash
# Code formatting
black src/ tests/
isort src/ tests/

# Type checking
mypy src/

# Linting
flake8 src/
```

### Next Development Steps (Optional Enhancements)

Core functionality complete! Optional future enhancements:

1. **Resume Functionality**:
   ```bash
   # Implement proper crawl state persistence
   # - src/crawler/persistence/crawl_state.py (currently stub)
   ```

2. **Advanced Analysis Features**:
   ```bash
   # Character relationship extraction using LLM
   # Network graph visualization
   # Export to different formats (CSV, GraphML)
   ```

3. **Performance Optimizations**:
   ```bash
   # Better session cleanup (fix aiohttp warnings)
   # Parallel processing for large wikias
   # Content deduplication
   ```

## Troubleshooting & Common Issues

### Windows Compatibility
- **Unicode Issues**: Log messages use `[OK]`, `[ERROR]`, `[INFO]` instead of Unicode symbols
- **Path Issues**: Project uses forward slashes; Windows handles them correctly
- **Console Encoding**: Avoid emojis/Unicode symbols in output (documented in CLAUDE.md)

### Test Suite Issues
```bash
# If pytest times out on rate limiting tests:
python -m pytest tests/test_crawler/utils/ -v  # Test smaller subset

# If coverage fails (expected with stubs):
python -m pytest tests/ --cov=src --cov-fail-under=20  # Lower threshold

# Quick verification test:
python -m pytest tests/test_crawler/utils/test_url_utils.py::TestURLUtilsValidation::test_validate_malformed_url -v
```

### Installation Issues
```bash
# If pip install fails:
pip install -e ".[dev]" --user  # Install to user directory

# If dependencies conflict:
pip install -e . --upgrade  # Install base package first
```

### Testing Issues
```bash
# If test_crawl.py shows "Pages crawled: 0" - THIS IS EXPECTED
# The content extraction pipeline is not implemented yet
# This validates the infrastructure is working correctly

# If project directories not created:
ls -la data/projects/  # Check if base directories exist
mkdir -p data/projects  # Create if needed
```

### Development Continuation

#### Priority Implementation Order:
1. **PageExtractor** (`src/crawler/extraction/page_extractor.py`) - Extract content from HTML
2. **WikiaParser** (`src/crawler/extraction/wikia_parser.py`) - Parse Wikia-specific content
3. **CrawlState** (`src/crawler/persistence/crawl_state.py`) - Save/load crawl state
4. **Main CLI** (`main.py`, `scripts/crawl_wikia.py`) - Working command interface

#### Validation After Each Step:
```bash
# After implementing PageExtractor:
python test_crawl.py  # Should show pages_crawled > 0

# After implementing CrawlState:
ls data/projects/avatar_test/crawl_state/  # Should have state files

# After implementing CLI:
python main.py crawl test_project https://avatar.fandom.com/wiki/Avatar_Wiki --max-pages 5
```

## Configuration
Configuration uses YAML files with hierarchical overrides. The system respects:
- Global crawler settings (rate limits, user agent)
- Per-domain rate limiting overrides
- Project-specific configuration
- Wikia namespace and content filters

### Configuration Files:
- `config/crawler_config.yaml` - Main crawler configuration
- `config/rate_limits.yaml` - Domain-specific rate limiting
- Project directories automatically created in `data/projects/<project_name>/`

## Current Capabilities âœ… FULLY WORKING

The WikiaAnalyzer can now:
1. **Extract real content** from any Fandom/Wikia site with rich character data
2. **Save human-readable files** like `Tenzin_20250911.json` instead of cryptic hashes
3. **Complete CLI interface** with crawl, status, list, and view commands
4. **Automatic categorization** into characters, articles, disambiguation pages
5. **Intelligent link discovery** finding hundreds of related pages automatically  
6. **Ethical crawling** with rate limiting (1.0s delays) and robots.txt compliance
7. **Structured JSON output** with titles, content, links, categories, and infobox data
8. **Project-based organization** with isolated storage per wikia site

**Ready for immediate use on any wikia site!**

## Contributing
This project uses modern Python development practices:
- **Type hints** throughout the codebase
- **Comprehensive testing** with pytest
- **Code formatting** with black and isort
- **Modular architecture** for independent component development

## License
MIT License (see LICENSE file)